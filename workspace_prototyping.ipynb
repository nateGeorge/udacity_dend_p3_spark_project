{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "First unzip data so we can load it in pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/log-data.zip\n",
      "  inflating: data/log_data/2018-11-01-events.json  \n",
      "  inflating: data/log_data/2018-11-02-events.json  \n",
      "  inflating: data/log_data/2018-11-03-events.json  \n",
      "  inflating: data/log_data/2018-11-04-events.json  \n",
      "  inflating: data/log_data/2018-11-05-events.json  \n",
      "  inflating: data/log_data/2018-11-06-events.json  \n",
      "  inflating: data/log_data/2018-11-07-events.json  \n",
      "  inflating: data/log_data/2018-11-08-events.json  \n",
      "  inflating: data/log_data/2018-11-09-events.json  \n",
      "  inflating: data/log_data/2018-11-10-events.json  \n",
      "  inflating: data/log_data/2018-11-11-events.json  \n",
      "  inflating: data/log_data/2018-11-12-events.json  \n",
      "  inflating: data/log_data/2018-11-13-events.json  \n",
      "  inflating: data/log_data/2018-11-14-events.json  \n",
      "  inflating: data/log_data/2018-11-15-events.json  \n",
      "  inflating: data/log_data/2018-11-16-events.json  \n",
      "  inflating: data/log_data/2018-11-17-events.json  \n",
      "  inflating: data/log_data/2018-11-18-events.json  \n",
      "  inflating: data/log_data/2018-11-19-events.json  \n",
      "  inflating: data/log_data/2018-11-20-events.json  \n",
      "  inflating: data/log_data/2018-11-21-events.json  \n",
      "  inflating: data/log_data/2018-11-22-events.json  \n",
      "  inflating: data/log_data/2018-11-23-events.json  \n",
      "  inflating: data/log_data/2018-11-24-events.json  \n",
      "  inflating: data/log_data/2018-11-25-events.json  \n",
      "  inflating: data/log_data/2018-11-26-events.json  \n",
      "  inflating: data/log_data/2018-11-27-events.json  \n",
      "  inflating: data/log_data/2018-11-28-events.json  \n",
      "  inflating: data/log_data/2018-11-29-events.json  \n",
      "  inflating: data/log_data/2018-11-30-events.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip data/log-data.zip -d data/log_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For some reason, the song_data zip has a folder song_data while log_data.zip is just the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/song-data.zip\n",
      "   creating: data/song_data/\n",
      "  inflating: data/song_data/.DS_Store  \n",
      "   creating: data/song_data/A/\n",
      "  inflating: data/song_data/A/.DS_Store  \n",
      "   creating: data/song_data/A/A/\n",
      "  inflating: data/song_data/A/A/.DS_Store  \n",
      "   creating: data/song_data/A/A/A/\n",
      "  inflating: data/song_data/A/A/A/TRAAAAW128F429D538.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAABD128F429CF47.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAADZ128F9348C2E.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAAEF128F4273421.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAAFD128F92F423A.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAAMO128F1481E7F.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAAMQ128F1460CD3.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAAPK128E0786D96.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAARJ128F9320760.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAAVG12903CFA543.json  \n",
      "  inflating: data/song_data/A/A/A/TRAAAVO128F93133D4.json  \n",
      "   creating: data/song_data/A/A/B/\n",
      "  inflating: data/song_data/A/A/B/TRAABCL128F4286650.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABDL12903CAABBA.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABJL12903CDCF1A.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABJV128F1460C49.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABLR128F423B7E3.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABNV128F425CEE1.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABRB128F9306DD5.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABVM128F92CA9DC.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABXG128F9318EBD.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABYN12903CFD305.json  \n",
      "  inflating: data/song_data/A/A/B/TRAABYW128F4244559.json  \n",
      "   creating: data/song_data/A/A/C/\n",
      "  inflating: data/song_data/A/A/C/TRAACCG128F92E8A55.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACER128F4290F96.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACFV128F935E50B.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACHN128F1489601.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACIW12903CC0F6D.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACLV128F427E123.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACNS128F14A2DF5.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACOW128F933E35F.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACPE128F421C1B9.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACQT128F9331780.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACSL128F93462F4.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACTB12903CAAF15.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACVS128E078BE39.json  \n",
      "  inflating: data/song_data/A/A/C/TRAACZK128F4243829.json  \n",
      "   creating: data/song_data/A/B/\n",
      "  inflating: data/song_data/A/B/.DS_Store  \n",
      "   creating: data/song_data/A/B/A/\n",
      "  inflating: data/song_data/A/B/A/TRABACN128F425B784.json  \n",
      "  inflating: data/song_data/A/B/A/TRABAFJ128F42AF24E.json  \n",
      "  inflating: data/song_data/A/B/A/TRABAFP128F931E9A1.json  \n",
      "  inflating: data/song_data/A/B/A/TRABAIO128F42938F9.json  \n",
      "  inflating: data/song_data/A/B/A/TRABATO128F42627E9.json  \n",
      "  inflating: data/song_data/A/B/A/TRABAVQ12903CBF7E0.json  \n",
      "  inflating: data/song_data/A/B/A/TRABAWW128F4250A31.json  \n",
      "  inflating: data/song_data/A/B/A/TRABAXL128F424FC50.json  \n",
      "  inflating: data/song_data/A/B/A/TRABAXR128F426515F.json  \n",
      "  inflating: data/song_data/A/B/A/TRABAXV128F92F6AE3.json  \n",
      "  inflating: data/song_data/A/B/A/TRABAZH128F930419A.json  \n",
      "   creating: data/song_data/A/B/B/\n",
      "  inflating: data/song_data/A/B/B/TRABBAM128F429D223.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBBV128F42967D7.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBJE12903CDB442.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBKX128F4285205.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBLU128F93349CF.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBNP128F932546F.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBOP128F931B50D.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBOR128F4286200.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBTA128F933D304.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBVJ128F92F7EAA.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBXU128F92FEF48.json  \n",
      "  inflating: data/song_data/A/B/B/TRABBZN12903CD9297.json  \n",
      "   creating: data/song_data/A/B/C/\n",
      "  inflating: data/song_data/A/B/C/TRABCAJ12903CDFCC2.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCEC128F426456E.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCEI128F424C983.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCFL128F149BB0D.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCIX128F4265903.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCKL128F423A778.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCPZ128F4275C32.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCRU128F423F449.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCTK128F934B224.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCUQ128E0783E2B.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCXB128F4286BD3.json  \n",
      "  inflating: data/song_data/A/B/C/TRABCYE128F934CE1D.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip data/song-data.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS', 'AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://62369c65a3e6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1b4717cba8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# s3 path\n",
    "#input_data = \"s3a://udacity-dend/\"\n",
    "# workspace path\n",
    "input_data = 'data'\n",
    "output_data = \"s3a://udacity-dend-spark-dwh/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 2 seconds\n"
     ]
    }
   ],
   "source": [
    "# get filepath to song data file\n",
    "song_data = os.path.join(input_data, 'song_data/*/*/*/*.json')\n",
    "\n",
    "# read song data file\n",
    "start = time.time()\n",
    "df = spark.read.json(song_data)\n",
    "end = time.time()\n",
    "print('took', int(end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist_id='ARDR4AC1187FB371A1', artist_latitude=None, artist_location='', artist_longitude=None, artist_name='Montserrat Caballé;Placido Domingo;Vicente Sardinero;Judith Blegen;Sherrill Milnes;Georg Solti', duration=511.16363, num_songs=1, song_id='SOBAYLL12A8C138AF9', title='Sono andati? Fingevo di dormire', year=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist_id',\n",
       " 'artist_latitude',\n",
       " 'artist_location',\n",
       " 'artist_longitude',\n",
       " 'artist_name',\n",
       " 'duration',\n",
       " 'num_songs',\n",
       " 'song_id',\n",
       " 'title',\n",
       " 'year']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# extract columns to create songs table\n",
    "songs_cols = ['song_id', 'title', 'artist_id', 'year', 'duration']\n",
    "songs_table = df.select(songs_cols).dropDuplicates()\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.mode('overwrite').partitionBy('year', 'artist_id').parquet(os.path.join(output_data, 'songs'))\n",
    "end = time.time()\n",
    "print('took', int(end-start), 's to write songs table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 424 s to write artists table\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# extract columns to create artists table\n",
    "artists_cols = ['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']\n",
    "artists_table = df.select(artists_cols).dropDuplicates()\n",
    "\n",
    "# write artists table to parquet files\n",
    "artists_table.write.mode('overwrite').parquet(os.path.join(output_data, 'artists'))\n",
    "end = time.time()\n",
    "print('took', int(end-start), 's to write artists table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0 seconds\n"
     ]
    }
   ],
   "source": [
    "# get filepath to log data file\n",
    "# on s3, paths are log_data/year/month/date.json\n",
    "# in workspace, path is logdata/date.json\n",
    "# log_data = os.path.join(input_data, 'log_data/*/*/*.json')\n",
    "log_data = os.path.join(input_data, 'log_data/*.json')\n",
    "\n",
    "# read log data file\n",
    "start = time.time()\n",
    "df = spark.read.json(log_data)\n",
    "end = time.time()\n",
    "print('took', int(end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Harmonia', auth='Logged In', firstName='Ryan', gender='M', itemInSession=0, lastName='Smith', length=655.77751, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016707796.0, sessionId=583, song='Sehr kosmisch', status=200, ts=1542241826796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist',\n",
       " 'auth',\n",
       " 'firstName',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'lastName',\n",
       " 'length',\n",
       " 'level',\n",
       " 'location',\n",
       " 'method',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'status',\n",
       " 'ts',\n",
       " 'userAgent',\n",
       " 'userId']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artist', 'string'),\n",
       " ('auth', 'string'),\n",
       " ('firstName', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('itemInSession', 'bigint'),\n",
       " ('lastName', 'string'),\n",
       " ('length', 'double'),\n",
       " ('level', 'string'),\n",
       " ('location', 'string'),\n",
       " ('method', 'string'),\n",
       " ('page', 'string'),\n",
       " ('registration', 'double'),\n",
       " ('sessionId', 'bigint'),\n",
       " ('song', 'string'),\n",
       " ('status', 'bigint'),\n",
       " ('ts', 'bigint'),\n",
       " ('userAgent', 'string'),\n",
       " ('userId', 'string')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 549 s to write users table\n"
     ]
    }
   ],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.filter(df['page'] == 'NextSong')\n",
    "\n",
    "start = time.time()\n",
    "# extract columns for users table\n",
    "users_cols = ['userId', 'firstName', 'lastName', 'gender', 'level']\n",
    "users_table = df.select(users_cols).dropDuplicates()\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.mode('overwrite').parquet(os.path.join(output_data, 'users'))\n",
    "end = time.time()\n",
    "print('took', int(end-start), 's to write users table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "# ts is timestamp in ms\n",
    "# this is not necessary since we don't use it\n",
    "# get_timestamp = udf(lambda x: x / 1000, TimestampType())\n",
    "# df = df.withColumn('ts_s', get_timestamp('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "# timestamp is is ms\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x / 1000), TimestampType())\n",
    "df = df.withColumn('start_time', get_datetime('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "\n",
    "time_table = df.select(\"start_time\").dropDuplicates() \\\n",
    "                        .withColumn(\"hour\", hour(col(\"start_time\"))) \\\n",
    "                        .withColumn(\"day\", dayofmonth(col(\"start_time\"))) \\\n",
    "                        .withColumn(\"week\", weekofyear(col(\"start_time\"))) \\\n",
    "                        .withColumn(\"month\", month(col(\"start_time\"))) \\\n",
    "                        .withColumn(\"year\", year(col(\"start_time\"))) \\\n",
    "                        .withColumn(\"weekday\", date_format(col(\"start_time\"), 'E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(start_time=datetime.datetime(2018, 11, 21, 6, 18, 12, 796000), hour=6, day=21, week=47, month=11, year=2018, weekday='Wed')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1683 s to write time table\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode('overwrite').partitionBy('year', 'month').parquet(os.path.join(output_data, 'time'))\n",
    "end = time.time()\n",
    "\n",
    "print('took', int(end-start), 's to write time table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 168 s to write songplays table\n"
     ]
    }
   ],
   "source": [
    "# read in song data to use for songplays table\n",
    "songs_df = spark.read.parquet(os.path.join(output_data, 'songs/*/*/*'))\n",
    "\n",
    "# read in artist data\n",
    "artists_df = spark.read.parquet(output_data + 'artists/*')\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table\n",
    "# columns desired: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "# we are joining the songs and artists data with logs to get the song_id and artist_id from the songs data\n",
    "# need to join on song title and artist name\n",
    "\n",
    "# first join songs and logs dfs on song title\n",
    "songs_logs_df = df.join(songs_df, (df.song == songs_df.title))\n",
    "# next join that df with artists on artist name\n",
    "artists_songs_logs_df = songs_logs_df.join(artists_df, (songs_logs_df.artist == artists_df.artist_name))\n",
    "songplay_cols = ['start_time', 'userId', 'level', 'song_id', 'artist_id', 'sessionId', 'location', 'userAgent']\n",
    "# calculate year and month from start_time -- probably faster than a join on the time table\n",
    "songplays_table = artists_songs_logs_df.select(songplay_cols) \\\n",
    "                    .withColumn('songplay_id', monotonically_increasing_id()) \\\n",
    "                    .withColumn(\"month\", month(col(\"start_time\"))) \\\n",
    "                    .withColumn(\"year\", year(col(\"start_time\")))\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "start = time.time()\n",
    "songplays_table.write.mode('overwrite').partitionBy('year', 'month').parquet(os.path.join(output_data, 'songplays'))\n",
    "end = time.time()\n",
    "print('took', int(end-start), 's to write songplays table')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
